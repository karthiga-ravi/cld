docker

sudo apt-get update
sudo apt install docker.io
sudo systemctl enable docker
sudo systemctl status docker     (-->active if not active use sudo systemctl start docker to get active)
nano Dockerfile
(FROM ubuntu:latest
 RUN apt-get update
 CMD ["echo","helloo all"])
sudo docker build -t sampleimg .
sudo docker images
sudo docker run sampleimg
sudo docker login
(username : karthigaravi
 pass: karthi200529)
sudo docker tag sampleimg:latest kerthigaravi/samp
sudo docker push karthigaravi/samp:latest
sudo docker images
sudo docker pull karthigaravi/samp:latest
sudo docker run karthigaravi/samp:latest (Same op as sudo docker run sampleimg this cmd)
sudo docker rmi sampleimg:latest(delete image)
sudo docker ps -a
sudo docker rm container_id(Removes the container)



sudo apt-get update
sudo apt install docker.io
sudo systemctl enable docker
sudo systemctl status docker                                                                                                                                                              nano Dockerfile                                                                                                                                                                              dockerfile FROM nginx:latest
COPY . /usr/share/nginx/html
EXPOSE 80
CMD ["nginx","-g","daemon off;"]                                                                                                                                                  docker build -t html-page .                                                                                                                                                                       docker run -d -p 8080:80 html-page                                                          
docker stop <container-id>

spark-shell --conf spark.driver.bindAddress=127.0.0.1 --conf spark.driver.host=127.0.0.1



spark
1. sudo apt update
2. java -version
3. sudo apt install scala
4. Download apache spark from https://www.apache.org/dyn/closer.lua/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz
5. cd Downloads
6. ls
7. tar xvf spark-3.5.3-bin-hadoop3.tgz
8. sudo mv spark-3.5.3-bin-hadoop3 /usr/local/spark
8. cd /usr/local/
9. ls
10. cd
11. nano ~/.bashrc
12. add ' export PATH=$PATH:/usr/local/spark/bin ' at the end and save -> exit
13. source ~/.bashrc
14. spark-shell
15. println("Hello,World!")
16. Create a file, input.txt in tce id (nano input.txt)
IN SPARK SHELL
17. val inputfile=sc.textFile("input.txt")
18. val counts = inputfile.flatMap(line => line.split(" ")).map(word=>(word,1)).reduceByKey(+)
19. counts.saveAsTextFile("output")
IN TCE ID
20. cd output
21. ls
22. cat part-00000



Hadoop

java -version
Hadoop version
ssh localhost
start-all.sh
nano input.txt(content of file, ctrl+o enter ctrl+X --> to save file)
hadoop fs -mkdir /mapreduce
hadoop fs -put input.txt /mapreduce
nano wrdcount.java(code for wrdcount,  ctrl+o enter ctrl+X --> to save file)
javac -classpath $(hadoop classpath) -d . wrdcount.java
jar cf wc.jar wrdcount*.class
hadoop jar wc.jar wrdcount /mapreduce/input.txt /mapreduce/output
hadoop fs -ls /mapreduce/output
hadoop fs -cat /mapreduce/output/part-r-00000                        

stop-all.sh